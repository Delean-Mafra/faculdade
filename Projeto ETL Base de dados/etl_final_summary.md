# ğŸ“Š Projeto ETL: Resumo Executivo

## ğŸ¯ Objetivo do Projeto

Implementar um pipeline **ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carregamento)** completo para dados de vendas, utilizando as melhores prÃ¡ticas de **Engenharia de Dados** com:

- **Apache Airflow** - OrquestraÃ§Ã£o de workflows
- **Apache Spark** - Processamento distribuÃ­do de dados
- **PostgreSQL** - Fonte de dados transacional
- **Hadoop HDFS** - Armazenamento de dados em camadas (Bronze/Silver/Gold)
- **Data Lake** - Arquitetura moderno de dados

---

## ğŸ“¦ Arquivos Entregues

### 1. **DAG Airflow** (`etl_airflow_dag.py`)
- OrquestraÃ§Ã£o completa do pipeline
- 9 tasks integradas
- Tratamento de erros e retries
- Envio de relatÃ³rios por email

### 2. **Scripts Spark** (4 arquivos)
- `spark_full_load.py` - Carga completa das tabelas
- `spark_incremental_load.py` - Carga incremental de orders
- `spark_silver_transform.py` - TransformaÃ§Ã£o e enriquecimento
- `spark_gold_load.py` - GeraÃ§Ã£o de relatÃ³rios finais

### 3. **Queries SQL** (ValidaÃ§Ã£o e AnÃ¡lises)
- `sql_validation_queries.sql` - ValidaÃ§Ãµes de integridade
- `etl_advanced_queries.sql` - AnÃ¡lises avanÃ§adas (10 categorias)

### 4. **Docker Compose** (`docker-compose.yml`)
- Stack completo prÃ©-configurado
- 6 serviÃ§os Docker
- Ambiente pronto para uso

### 5. **Testes e ValidaÃ§Ã£o**
- `etl_test_validation.py` - Suite completa de testes
- `etl_jupyter_notebook.py` - ExploraÃ§Ã£o interativa

### 6. **DocumentaÃ§Ã£o**
- `README.md` - InstruÃ§Ãµes prÃ¡ticas
- `INSTALLATION_GUIDE.md` - Guia detalhado
- Este arquivo - Resumo executivo

---

## ğŸ—ï¸ Arquitetura do Data Lake

```
/data_lake/
â”œâ”€â”€ bronze/                 # Dados brutos, sem transformaÃ§Ã£o
â”‚   â”œâ”€â”€ customers/         # Carga full diÃ¡ria
â”‚   â”œâ”€â”€ products/          # Carga full diÃ¡ria
â”‚   â”œâ”€â”€ orders/            # Carga incremental
â”‚   â””â”€â”€ order_items/       # Carga full diÃ¡ria
â”‚
â”œâ”€â”€ silver/                # Dados limpos e enriquecidos
â”‚   â””â”€â”€ orders_enriched/   # Pedidos com clientes e produtos
â”‚
â””â”€â”€ gold/                  # Dados prontos para anÃ¡lise
    â”œâ”€â”€ vendas_por_dia/    # AgregaÃ§Ã£o diÃ¡ria
    â”œâ”€â”€ maiores_vendas/    # Top 10 vendas
    â”œâ”€â”€ produtos_vendidos/ # Performance de produtos
    â””â”€â”€ estoque_baixo/     # Alertas de inventÃ¡rio
```

---

## ğŸ”„ Fluxo de Dados

```
PostgreSQL (Fonte)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         APACHE AIRFLOW                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Task 1-3: Carga Full (S3S)         â”‚ â”‚
â”‚  â”‚  - customers, products, order_items â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â†“                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Task 4: Carga Incremental           â”‚â”‚
â”‚  â”‚  - orders (filtrado por data)        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                 â†“                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Task 5: ValidaÃ§Ã£o (Bronze Layer)    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                 â†“                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Task 6: TransformaÃ§Ã£o (Silver)      â”‚â”‚
â”‚  â”‚  - Limpeza, enriquecimento com joins â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                 â†“                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Task 7: Carregamento (Gold)         â”‚â”‚
â”‚  â”‚  - 4 relatÃ³rios preparados           â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                 â†“                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Task 8-9: GeraÃ§Ã£o e Envio           â”‚â”‚
â”‚  â”‚  - RelatÃ³rio por email               â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
Hadoop HDFS (Data Lake)
       â†“
AnÃ¡lises, BI, RelatÃ³rios
```

---

## ğŸ“‹ Tabelas PostgreSQL

### 1. **customers**
```sql
customer_id (PK), customer_name, email, phone, city, created_at
```

### 2. **products**
```sql
product_id (PK), product_name, category, price, stock_quantity, created_at
```

### 3. **orders**
```sql
order_id (PK), customer_id (FK), order_date, total_amount, status, created_at
```

### 4. **order_items**
```sql
order_item_id (PK), order_id (FK), product_id (FK), quantity, unit_price, total_price, created_at
```

---

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: Docker Compose (Recomendado - 5 min)

```bash
# Clone ou prepare o projeto
cd projeto-etl

# Inicie com um comando
docker-compose up -d

# Aguarde ~2 minutos

# Acesse: http://localhost:8080
# UsuÃ¡rio: admin | Senha: admin

# Dispare a DAG no Airflow
```

### OpÃ§Ã£o 2: InstalaÃ§Ã£o Local (15-20 min)

```bash
# 1. Instalar dependÃªncias (ver INSTALLATION_GUIDE.md)
# 2. Configurar PostgreSQL com dados
# 3. Iniciar Hadoop HDFS
# 4. Inicializar Airflow
# 5. Disparar DAG

# Ver documentaÃ§Ã£o detalhada no README.md
```

---

## ğŸ” ValidaÃ§Ã£o de Dados

### Camada Bronze
- Dados brutos do PostgreSQL
- Snapshots diÃ¡rios por tabela
- Estrutura particionada por data

### Camada Silver
- Dados limpos e deduplica
- Joins entre tabelas
- ValidaÃ§Ãµes de integridade

### Camada Gold
- 4 relatÃ³rios principais:
  1. **Vendas por Dia** - AgregaÃ§Ã£o diÃ¡ria
  2. **Maiores Vendas** - Top 10 transaÃ§Ãµes
  3. **Produtos Vendidos** - Performance por produto
  4. **Estoque Baixo** - Alertas de inventÃ¡rio

---

## ğŸ“Š RelatÃ³rios Gerados

### 1. **Dashboard de Vendas DiÃ¡rias**
- Total de vendas por dia
- NÃºmero de pedidos
- Quantidade de clientes
- Ticket mÃ©dio

### 2. **AnÃ¡lise de Produtos**
- Produtos mais vendidos
- Produtos com baixo estoque
- Produtos sem rotatividade
- Performance por categoria

### 3. **SegmentaÃ§Ã£o de Clientes**
- Clientes por valor gasto
- Clientes inativos
- SegmentaÃ§Ã£o RFM (RecÃªncia/FrequÃªncia/MonetÃ¡rio)

### 4. **Alertas Operacionais**
- Estoque crÃ­tico (< 10 unidades)
- Produtos parados
- Clientes em risco

---

## âœ… Checklist de VerificaÃ§Ã£o

### Antes de Executar
- [ ] Docker instalado (se usar opÃ§Ã£o 1)
- [ ] Python 3.8+ instalado
- [ ] 8GB RAM disponÃ­vel
- [ ] Portas 5432, 8080, 9000, 9870 livres

### ApÃ³s Iniciar
- [ ] PostgreSQL conecta e tem dados
- [ ] Hadoop HDFS criou estrutura /data_lake
- [ ] Airflow acessÃ­vel em http://localhost:8080
- [ ] Arquivo JDBC PostgreSQL em $SPARK_HOME/jars/

### Antes de Disparar DAG
- [ ] ConexÃ£o PostgreSQL configurada no Airflow
- [ ] Scripts Spark no local correto
- [ ] VariÃ¡veis Airflow definidas
- [ ] Email configurado (opcional)

### ApÃ³s ExecuÃ§Ã£o
- [ ] Todas as tasks concluÃ­das (âœ“)
- [ ] Dados em /data_lake/gold gerados
- [ ] Email recebido com relatÃ³rio (opcional)
- [ ] Nenhum erro crÃ­tico nos logs

---

## ğŸ› ï¸ SoluÃ§Ã£o de Problemas

| Problema | SoluÃ§Ã£o |
|----------|---------|
| PostgreSQL nÃ£o conecta | Verificar credenciais e porta 5432 |
| HDFS connection refused | Iniciar Hadoop: `$HADOOP_HOME/sbin/start-dfs.sh` |
| Spark driver not found | Copiar postgresql-42.5.0.jar para $SPARK_HOME/jars/ |
| DAG nÃ£o aparece no Airflow | Verificar sintaxe e permissÃµes em ~/airflow/dags/ |
| Task falha com path error | Atualizar caminhos na DAG conforme seu ambiente |
| Email nÃ£o envia | Configurar SMTP e habilitar autenticaÃ§Ã£o |

---

## ğŸ“š PrÃ³ximos Passos

1. **Adicionar Novos RelatÃ³rios**
   - Editar `spark_gold_load.py`
   - Adicionar novas aggregations
   - Testar antes de produÃ§Ã£o

2. **Escalar para ProduÃ§Ã£o**
   - Usar cluster Spark dedicado
   - Implementar alertas e monitoramento
   - Adicionar backup automÃ¡tico

3. **Melhorar Performance**
   - Criar Ã­ndices no PostgreSQL
   - Particionar tabelas HDFS
   - Otimizar queries SQL

4. **Integrar com BI**
   - Conectar Tableau/Power BI Ã  camada Gold
   - Criar dashboards interativos
   - Implementar alertas automÃ¡ticos

5. **AutomaÃ§Ã£o AvanÃ§ada**
   - Schedule para rodar automaticamente
   - CI/CD para deployments
   - Testes unitÃ¡rios e integraÃ§Ã£o

---

## ğŸ“ Suporte

### ReferÃªncias Ãšteis
- [DocumentaÃ§Ã£o Apache Airflow](https://airflow.apache.org/)
- [DocumentaÃ§Ã£o Apache Spark](https://spark.apache.org/docs/latest/)
- [Hadoop HDFS Guide](https://hadoop.apache.org/docs/r3.2.1/)
- [PostgreSQL Manual](https://www.postgresql.org/docs/)

### Comunidades
- Stack Overflow: tag `apache-airflow`, `apache-spark`
- Reddit: r/dataengineering, r/databases
- GitHub: Issues em projetos oficiais

---

## ğŸ“Š EstatÃ­sticas do Projeto

| MÃ©trica | Valor |
|---------|-------|
| **Linhas de CÃ³digo** | ~2.500 |
| **Arquivos Entregues** | 10+ |
| **Tasks Airflow** | 9 |
| **Scripts Spark** | 4 |
| **Queries SQL** | 30+ |
| **Tabelas PostgreSQL** | 4 |
| **Camadas Data Lake** | 3 |
| **Testes Automatizados** | 12+ |

---

## ğŸ“ Aprendizados Principais

âœ… Arquitetura de Data Lake (Bronze/Silver/Gold)
âœ… ETL completo com Spark e Airflow
âœ… ExtraÃ§Ã£o incremental vs full
âœ… TransformaÃ§Ã£o e enriquecimento de dados
âœ… ValidaÃ§Ã£o e qualidade de dados
âœ… OrquestraÃ§Ã£o de workflows complexos
âœ… Best practices de Engenharia de Dados
âœ… RelatÃ³rios automÃ¡ticos por email

---

## ğŸ“ ConclusÃ£o

Este projeto apresenta uma soluÃ§Ã£o **production-ready** para pipelines ETL, cobrindo toda a cadeia de Engenharia de Dados moderna:

- **ExtraÃ§Ã£o** de mÃºltiplas fontes
- **TransformaÃ§Ã£o** e limpeza de dados
- **Carregamento** em camadas otimizadas
- **ValidaÃ§Ã£o** de integridade
- **AutomaÃ§Ã£o** e orquestraÃ§Ã£o
- **GeraÃ§Ã£o** de insights acionÃ¡veis

Todos os componentes estÃ£o **testados e prontos** para uso em ambientes de desenvolvimento e, com ajustes mÃ­nimos, em produÃ§Ã£o.

---

**Bom trabalho e sucesso com seu projeto ETL! ğŸš€**

*Ãšltima atualizaÃ§Ã£o: 2025*