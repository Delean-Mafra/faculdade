# âš¡ Quick Start - Projeto ETL em 5 Minutos

## ğŸš€ Caminho Mais RÃ¡pido: Docker Compose

### PrÃ©-requisito Ãšnico
- Docker + Docker Compose instalado

### Passos (Copie e Cole)

#### 1ï¸âƒ£ Clone o Projeto
```bash
git clone <seu-repo>
cd projeto-etl
```

#### 2ï¸âƒ£ Inicie Tudo
```bash
docker-compose up -d
```

#### 3ï¸âƒ£ Aguarde (2-3 minutos)
```bash
# Ver status
docker-compose ps

# Ver logs
docker-compose logs -f airflow_webserver
```

#### 4ï¸âƒ£ Acesse Airflow
```
http://localhost:8080
UsuÃ¡rio: admin
Senha: admin
```

#### 5ï¸âƒ£ Dispare a DAG
```
1. Clique na DAG: etl_vendas_pipeline
2. Clique no toggle (ON)
3. Clique em "Trigger DAG"
4. Acompanhe a execuÃ§Ã£o
```

---

## ğŸ“Š O que Acontece Automaticamente

```
â”œâ”€ PostgreSQL inicia com dados de exemplo
â”œâ”€ Hadoop HDFS cria estrutura /data_lake
â”œâ”€ Airflow orquestra 9 tasks em sequÃªncia
â”‚  â”œâ”€ Extrai 4 tabelas (customers, products, orders, order_items)
â”‚  â”œâ”€ Valida dados (contagem, integridade)
â”‚  â”œâ”€ Transforma e enriquece (joins, limpeza)
â”‚  â”œâ”€ Gera 4 relatÃ³rios (vendas, produtos, estoque, anÃ¡lises)
â”‚  â””â”€ Envia por email (opcional)
â””â”€ Dados prontos em /data_lake/gold/
```

---

## ğŸ” Verificar Dados

### PostgreSQL (Dados de Origem)
```bash
docker-compose exec postgres psql -U postgres -d vendas_db
```

```sql
-- No prompt do psql:
SELECT COUNT(*) as total_clientes FROM customers;
SELECT COUNT(*) as total_pedidos FROM orders;
SELECT SUM(total_amount) as receita FROM orders;
\q
```

### Hadoop HDFS (Data Lake)
```bash
docker-compose exec namenode hdfs dfs -ls /data_lake/bronze/
docker-compose exec namenode hdfs dfs -ls /data_lake/gold/
```

### Airflow Web UI
```
http://localhost:8080/home

Abas Ãºteis:
- DAGs: Ver todas as pipelines
- Grid: Ver timeline de execuÃ§Ã£o
- Task Logs: Ver detalhes de cada task
```

---

## ğŸ“ˆ Exemplo de Resultado Esperado

ApÃ³s execuÃ§Ã£o bem-sucedida, vocÃª terÃ¡:

### 1. Bronze Layer (Raw Data)
```
/data_lake/bronze/
â”œâ”€â”€ customers/2025-01-15/  â†’ 3 registros
â”œâ”€â”€ products/2025-01-15/   â†’ 5 registros
â”œâ”€â”€ orders/2025-01-15/     â†’ 3 registros
â””â”€â”€ order_items/2025-01-15/ â†’ 6 registros
```

### 2. Silver Layer (Clean Data)
```
/data_lake/silver/
â””â”€â”€ orders_enriched/2025-01-15/
    â””â”€â”€ dados com clientes e produtos unidos
```

### 3. Gold Layer (Business Reports)
```
/data_lake/gold/
â”œâ”€â”€ vendas_por_dia/2025-01-15/
â”‚   â†’ Total vendas: R$ 5.100,00
â”‚   â†’ NÃºmero de pedidos: 3
â”‚
â”œâ”€â”€ maiores_vendas/2025-01-15/
â”‚   â†’ Top 1: JoÃ£o Silva - R$ 3.650,00
â”‚   â†’ Top 2: Maria Santos - R$ 1.350,00
â”‚
â”œâ”€â”€ produtos_vendidos/2025-01-15/
â”‚   â†’ Notebook Dell: 1 unidade (R$ 3.500,00)
â”‚   â†’ Webcam HD: 2 unidades (R$ 500,00)
â”‚
â””â”€â”€ estoque_baixo/2025-01-15/
    â†’ Teclado MecÃ¢nico: 8 unidades (âš ï¸ crÃ­tico)
    â†’ Monitor LG: 5 unidades (âš ï¸ crÃ­tico)
```

---

## ğŸ¯ Casos de Uso PrÃ¡ticos

### Usar Case 1: Visualizar Vendas do Dia
```bash
# 1. Acesse http://localhost:8080
# 2. VÃ¡ para DAG â†’ etl_vendas_pipeline
# 3. Veja task "load_gold_layer" 
# 4. Dados em /data_lake/gold/vendas_por_dia/
```

### Use Case 2: Checar Estoque Baixo
```bash
# Verifique automaticamente via task "load_gold_layer"
# Dados em /data_lake/gold/estoque_baixo/
# Alerta: Produtos com < 10 unidades
```

### Use Case 3: AnÃ¡lise de Clientes
```bash
# Jupyter Notebook (ver etl_jupyter_notebook.py)
# SegmentaÃ§Ã£o RFM (RecÃªncia, FrequÃªncia, MonetÃ¡rio)
# Identificar clientes-chave e em risco
```

### Use Case 4: Auditar Qualidade
```bash
# Via SQL: 
docker-compose exec postgres psql -U postgres -d vendas_db -f sql/validation_queries.sql

# Via Spark:
python3 etl_test_validation.py
```

---

## ğŸ”§ Comandos Ãšteis

### Gerenciar Containers
```bash
# Ver status
docker-compose ps

# Ver logs em tempo real
docker-compose logs -f airflow_scheduler

# Parar
docker-compose stop

# Reiniciar
docker-compose restart

# Parar e remover
docker-compose down

# Remover volumes (limpar tudo)
docker-compose down -v
```

### Executar Comandos Dentro dos Containers
```bash
# PostgreSQL
docker-compose exec postgres psql -U postgres -d vendas_db -c "SELECT COUNT(*) FROM orders;"

# HDFS
docker-compose exec namenode hdfs dfs -cat /data_lake/gold/vendas_por_dia/*

# Spark
docker-compose exec namenode spark-shell -c "spark.read.parquet('/data_lake/gold/vendas_por_dia').show()"
```

### Ver Logs Detalhados
```bash
# Airflow Webserver
docker-compose logs airflow_webserver | tail -50

# Airflow Scheduler
docker-compose logs airflow_scheduler | tail -50

# PostgreSQL
docker-compose logs postgres | tail -50

# Todos os containers
docker-compose logs --follow
```

---

## ğŸ› Troubleshooting RÃ¡pido

| Sintoma | SoluÃ§Ã£o |
|---------|---------|
| "Connection refused" | Aguarde 2-3 min, containers iniciando |
| DAG nÃ£o aparece | `docker-compose restart airflow_scheduler` |
| Task falha | Ver logs: http://localhost:8080 â†’ DAG â†’ Task â†’ Logs |
| PostgreSQL vazio | Dados carregam automÃ¡tico no init |
| HDFS inacessÃ­vel | `docker-compose logs namenode` |
| Airflow lento | Aumentar memÃ³ria Docker (Settings) |

---

## ğŸ“‹ Estrutura de Arquivos Esperada

ApÃ³s clone, vocÃª deve ter:

```
projeto-etl/
â”œâ”€â”€ docker-compose.yml          âœ… OrquestraÃ§Ã£o Docker
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_vendas_pipeline.py  âœ… DAG Airflow
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ spark_full_load.py      âœ… Carga full
â”‚   â”œâ”€â”€ spark_incremental_load.py âœ… Carga incremental
â”‚   â”œâ”€â”€ spark_silver_transform.py âœ… TransformaÃ§Ã£o
â”‚   â””â”€â”€ spark_gold_load.py      âœ… RelatÃ³rios
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql       âœ… Schema PostgreSQL
â”‚   â”œâ”€â”€ insert_sample_data.sql  âœ… Dados iniciais
â”‚   â””â”€â”€ validation_queries.sql  âœ… Queries de teste
â”œâ”€â”€ config/
â”‚   â””â”€â”€ airflow_connections.yaml âœ… ConfiguraÃ§Ãµes
â”œâ”€â”€ README.md                    âœ… DocumentaÃ§Ã£o
â””â”€â”€ etl_test_validation.py      âœ… Testes

Todos fornecidos neste projeto!
```

---

## ğŸ“ Aprendizados em PrÃ¡tica

Ao executar este projeto, vocÃª pratica:

âœ… **OrquestraÃ§Ã£o** - Airflow orquestraÃ§Ã£o de tasks
âœ… **Spark SQL** - PySpark dataframes e queries
âœ… **Engenharia de Dados** - ETL completo
âœ… **Data Lake** - Bronze/Silver/Gold layers
âœ… **Docker** - ContainerizaÃ§Ã£o e composiÃ§Ã£o
âœ… **PostgreSQL** - Banco relacional
âœ… **HDFS** - Sistema de arquivos distribuÃ­do
âœ… **ValidaÃ§Ã£o** - Testes de qualidade de dados
âœ… **Automation** - Workflows automÃ¡ticos
âœ… **RelatÃ³rios** - GeraÃ§Ã£o de insights

---

## ğŸ“ PrÃ³ximos Passos

### Imediato (ApÃ³s 1Âª ExecuÃ§Ã£o)
1. âœ… Executar DAG e ver tudo funcionar
2. âœ… Explorar dados em PostgreSQL
3. âœ… Ver arquivos em HDFS
4. âœ… Verificar logs no Airflow

### Curto Prazo (1-2 horas)
1. ğŸ“– Ler comentÃ¡rios nos scripts
2. ğŸ“Š Executar queries SQL para anÃ¡lise
3. ğŸ” Explorar com Jupyter Notebook
4. ğŸ§ª Rodar testes de validaÃ§Ã£o

### MÃ©dio Prazo (1-2 dias)
1. ğŸ¨ Customizar relatÃ³rios
2. ğŸ“§ Configurar email real
3. ğŸ“ˆ Adicionar novos KPIs
4. ğŸš€ Preparar para produÃ§Ã£o

### Longo Prazo
1. ğŸ—ï¸ Escalar para dados reais
2. ğŸ“Š Integrar com BI (Tableau/Power BI)
3. ğŸ”” Implementar alertas
4. ğŸ¤– AutomaÃ§Ã£o avanÃ§ada

---

## â­ Dicas Profissionais

### 1. Monitoramento
```bash
# Ver execuÃ§Ã£o em tempo real
watch 'docker-compose ps'

# Monitorar recursos
docker stats
```

### 2. Desenvolvimento
```bash
# Testar script Spark localmente
spark-submit scripts/spark_full_load.py --help

# Validar DAG Airflow
python3 -m py_compile dags/etl_vendas_pipeline.py
```

### 3. Debugging
```bash
# Ver variÃ¡veis de ambiente
docker-compose exec airflow_webserver printenv

# Acessar bash do container
docker-compose exec postgres bash

# Inspecionar arquivo Parquet
docker-compose exec namenode spark-shell
```

### 4. Performance
```bash
# Aumentar worker threads
# Editar docker-compose.yml:
# AIRFLOW__CORE__PARALLELISM: 32

# Aumentar memÃ³ria Spark
# Editar DAG: conf={'spark.executor.memory': '4g'}
```

---

## ğŸ¯ Checklist Final

Antes de considerar "pronto":

- [ ] Docker Compose rodando sem erros
- [ ] Airflow acessÃ­vel em http://localhost:8080
- [ ] DAG `etl_vendas_pipeline` visÃ­vel
- [ ] Primeira execuÃ§Ã£o concluÃ­da (âœ“ em todas as tasks)
- [ ] Dados em PostgreSQL verificados
- [ ] Dados em HDFS (/data_lake) verificados
- [ ] RelatÃ³rios em gold/ gerados
- [ ] Nenhum erro crÃ­tico nos logs
- [ ] (Opcional) Email recebido com relatÃ³rio

---

## ğŸ‰ ParabÃ©ns!

VocÃª agora tem um **projeto ETL production-ready** totalmente funcional! 

### O que vocÃª conquistou:
âœ… Pipeline ETL completo  
âœ… Arquitetura moderna de dados  
âœ… AutomaÃ§Ã£o com Airflow  
âœ… Processamento com Spark  
âœ… Data Lake em 3 camadas  
âœ… RelatÃ³rios automÃ¡ticos  
âœ… Testes de qualidade  

### Agora vocÃª pode:
- ğŸ“Š Adicionar novos relatÃ³rios
- ğŸš€ Escalar para produÃ§Ã£o
- ğŸ¤– Automatizar decisÃµes
- ğŸ“ˆ Gerar insights em tempo real
- ğŸ¯ Transformar dados em valor

---

**Tempo total esperado: 5-10 minutos âš¡**

**Sucesso! Happy Data Engineering! ğŸš€**
